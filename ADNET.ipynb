{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import glob\n",
    "import re\n",
    "import random\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from tensorflow.keras.layers import Conv2D, Input, BatchNormalization, Lambda, ReLU, LeakyReLU, MaxPool2D, Flatten, Dense, Concatenate, Dropout, LayerNormalization\n",
    "from tensorflow.keras.activations import relu\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easydict import EasyDict as edict\n",
    "config = edict()\n",
    "\n",
    "config.TRAIN_DATA_PATH = '/Users/vijay/Downloads/Code_Data/ADNet/Data/'\n",
    "config.GENERATED_DATA_PATH = '/Users/vijay/Downloads/Code_Data/ADNet/generated_data/'\n",
    "config.LABEL_DIRS_TRAINED_TXT_PATH = '/Users/vijay/Downloads/Code_Data/ADNet/SL_PRETRAIN_LABELS_READ.txt'\n",
    "config.FINAL_WEIGHTS_DIR = '/Users/vijay/Downloads/Code_Data/ADNet/final_weights/'\n",
    "config.ADNET_CKPT_DIR = '/Users/vijay/Downloads/Code_Data/ADNet/checkpoints/'\n",
    "config.ONLINE_TUNE_TEST_SEQ_PATH = '/Users/vijay/Downloads/Code_Data/ADNet/test_seq/'\n",
    "config.ACTION_HISTORY_SHAPE = [1, 110]\n",
    "config.GAMMA = 0.9\n",
    "config.UNIFORM_TRANSLATION = 0.9\n",
    "config.UNIFORM_SCALE = 10\n",
    "\n",
    "config.SL_PRETRAIN_EPOCHS = 300\n",
    "config.SL_PRETRAIN_BATCH_SIZE = 10\n",
    "config.SL_POS_SAMPS_THRESHOLD = 0.7\n",
    "config.SL_NEG_SAMPS_THRESHOLD = 0.3\n",
    "\n",
    "config.RL_PRETRAIN_BATCH_SIZE = 128\n",
    "config.NUM_FRAMES_IN_PRETRAIN_RL = 10\n",
    "config.RL_POS_SAMPS_THRESHOLD = 0.7\n",
    "config.RL_NEG_SAMPS_THRESHOLD = 0.3\n",
    "\n",
    "\n",
    "config.REDECTION_NUM_SAMPLES = 256\n",
    "config.ONLINE_FINETUNE_BATCH_SIZE = 256\n",
    "config.ONLINE_FINETUNE_NUM_SAMPLES = 250\n",
    "config.ONLINE_FINETUNE_FREQUENCY = 10\n",
    "config.ONLINE_FINETUNE_NUM_EPOCHS = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "################################################\n",
    "## GENERATE TRAINING SAMPLES TO PRE TRAIN ADNET ####\n",
    "################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GENERATE_SAMPLES:\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.GENERATED_DATA_PATH = config.GENERATED_DATA_PATH\n",
    "        self.TRAIN_DATA_PATH = config.TRAIN_DATA_PATH # '/Users/vijay/Downloads/Code_Data/ADNet/Data/'\n",
    "#         self.TRAIN_GT_PATH  = '/Users/vijay/Downloads/Code_Data/ADNet/Data/'\n",
    "        self.NUM_ACTIONS = 11 # LEFT, RIGHT, UP, DOWN, DOUBLE_LEFT, DOUBLE_RIGHT, DOUBLE_UP, DOUBLE_DOWN, SCLAE_UP, SCALE_DOWN, STOP\n",
    "        self.NUM_CLASSES = 2 # OBJECT, BACKGROUND\n",
    "        self.REQ_ACTION_HISTORY = 10 # Last 10 values of actions are required\n",
    "        self.alpha = 0.03\n",
    "        self.ACTIONS = np.array([\n",
    "                                    [-1, 0, 0, 0], # left\n",
    "                                    [-2, 0, 0, 0], # double left\n",
    "                                    [+1, 0, 0, 0], # right\n",
    "                                    [+2, 0, 0, 0], # double right\n",
    "                                    [0, -1, 0, 0], # up\n",
    "                                    [0, -2, 0, 0], # double up \n",
    "                                    [0, +1, 0, 0], # down\n",
    "                                    [0, +2, 0, 0], # double down\n",
    "                                    [0, 0, 0, 0], # stop\n",
    "                                    [0, 0, -1, -1], # scale up\n",
    "                                    [0, 0, 1, 1] # sclae down\n",
    "                                 ], dtype=np.float16)   # SCALE_DOWN\n",
    "        \n",
    "    \n",
    "    ################################\n",
    "    \n",
    "    def get_gt_values(self, gt_path):\n",
    "        with open(gt_path, 'r') as f:\n",
    "\n",
    "            lines = f.readlines()\n",
    "\n",
    "            boxes = []\n",
    "            for line in lines:\n",
    "\n",
    "                if not line.strip():\n",
    "                    continue\n",
    "                x, y, w, h = [int(x) for x in line.split(',')]\n",
    "\n",
    "                boxes.append([x, y, w, h])\n",
    "\n",
    "            return boxes\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    ################################\n",
    "    \n",
    "    # https://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/\n",
    "    def calculate_IOU(self, box_1, box_2):\n",
    "        b1_x1, b1_y1, b1_x2, b1_y2 = box_1[0], box_1[1], box_1[0] + box_1[2], box_1[1] + box_1[3] # x, y, x+w = x_max, y + w = y_max\n",
    "        b2_x1, b2_y1, b2_x2, b2_y2 = box_2[0], box_2[1], box_2[0] + box_2[2], box_2[1] + box_2[3]\n",
    "        \n",
    "        \n",
    "        i_x1 = max(b1_x1, b2_x1)\n",
    "        i_y1 = max(b1_y1, b2_y1)\n",
    "        i_x2 = min(b1_x2, b2_x2)\n",
    "        i_y2 = min(b1_y2, b2_y2)\n",
    "        \n",
    "        \n",
    "        intersection_area = max(0, i_x2 - i_x1 + 1) * max(0, i_y2 - i_y1 + 1)\n",
    "        \n",
    "        \n",
    "        box1_area = (b1_x2 - b1_x1 + 1) * (b1_y2 - b1_y1 + 1)\n",
    "        box2_area = (b2_x2 - b2_x1 + 1) * (b2_y2 - b2_y1 + 1)\n",
    "        \n",
    "        iou = intersection_area / float(box1_area + box2_area - intersection_area)\n",
    "        \n",
    "        return iou\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    ################################\n",
    "    \n",
    "    def add_gaussian_noise(self, img_path):\n",
    "        img = cv2.imread(img_path)\n",
    "        img_shape = np.shape(img)\n",
    "        mean = 0\n",
    "        var = 10\n",
    "        sigma = var ** 0.5\n",
    "        gaussian = np.random.normal(mean, sigma, (img_shape[0], img_shape[1])) #  np.zeros((224, 224), np.float32)\n",
    "\n",
    "        noisy_image = np.zeros(img_shape, np.float32)\n",
    "\n",
    "        if len(img.shape) == 2:\n",
    "            noisy_image = img + gaussian\n",
    "        else:\n",
    "            noisy_image[:, :, 0] = img[:, :, 0] + gaussian\n",
    "            noisy_image[:, :, 1] = img[:, :, 1] + gaussian\n",
    "            noisy_image[:, :, 2] = img[:, :, 2] + gaussian\n",
    "\n",
    "        cv2.normalize(noisy_image, noisy_image, 0, 255, cv2.NORM_MINMAX, dtype=-1)\n",
    "        noisy_image = noisy_image.astype(np.uint8)\n",
    "\n",
    "        return noisy_image\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ###########################\n",
    "    \n",
    "    def get_noisy_boxes_wrt_gt_box_value(self, gt_values, noise_type, num_samples):\n",
    "        \n",
    "        x = gt_values[0]\n",
    "        y = gt_values[1]\n",
    "        w = gt_values[2]\n",
    "        h = gt_values[3]\n",
    "        \n",
    "        noisy_samples = []\n",
    "        if noise_type == 'gaussian':\n",
    "            \n",
    "    \n",
    "            cov_matrix = np.diag([pow((0.3 * w), 2), pow((0.3 * h), 2), pow((0.1 * w), 2), pow((0.1 * h), 2)])\n",
    "\n",
    "            for i in range(num_samples):\n",
    "                noisy_value = np.add(gt_values, np.random.multivariate_normal([0, 0, 0, 0], cov_matrix, 1)).astype(np.int16)\n",
    "#                 iou = self.calculate_IOU(noisy_value[0], gt_values)\n",
    "            \n",
    "            \n",
    "                noisy_samples.append(noisy_value[0])\n",
    "            return noisy_samples\n",
    "        \n",
    "        elif noise_type == 'uniform':\n",
    "            mean_wh = (w + h) * 0.5\n",
    "            centre_x, centre_y = x + (w * 0.5), y + (h * 0.5)\n",
    "            \n",
    "            for _ in range(num_samples):\n",
    "                dx = config.UNIFORM_TRANSLATION * mean_wh * random.uniform(-1.0, 1.0)\n",
    "                dy = config.UNIFORM_TRANSLATION * mean_wh * random.uniform(-1.0, 1.0)\n",
    "                dwh = 1.05 ** (config.UNIFORM_SCALE * random.uniform(-1.0, 1.0))\n",
    "\n",
    "\n",
    "                new_centre_x, new_centre_y = centre_x + dx, centre_y + dy\n",
    "                new_width, new_height = int(round(w * dwh)), int(round(h * dwh))\n",
    "\n",
    "                new_x = int(round(new_centre_x - (new_width * 0.5)))\n",
    "                new_y = int(round(new_centre_y - (new_height * 0.5))) \n",
    "                noisy_box = [new_x, new_y, new_width, new_height]\n",
    "                noisy_samples.append(noisy_box)\n",
    "            return noisy_samples\n",
    "      \n",
    "    \n",
    "    \n",
    "    \n",
    "    ################################\n",
    "    \n",
    "    def get_pos_neg_noisy_boxes(self, gt_values, img_path, num_of_samples, pos_threshold, neg_threshold):\n",
    "        num_pos_samples = int(num_of_samples * 0.5)\n",
    "        num_neg_samples = num_of_samples - num_pos_samples\n",
    "        \n",
    "        all_gaussian_samples = self.get_noisy_boxes_wrt_gt_box_value(gt_values, 'gaussian', num_pos_samples * 3)\n",
    "        all_uniform_samples = self.get_noisy_boxes_wrt_gt_box_value(gt_values, 'uniform', num_neg_samples * 3)\n",
    "        \n",
    "        all_pos_samples = [sample for sample in all_gaussian_samples if self.calculate_IOU(gt_values, sample) >= pos_threshold]\n",
    "        all_neg_samples = [sample for sample in all_uniform_samples if self.calculate_IOU(gt_values, sample) < neg_threshold]\n",
    "        \n",
    "        if len(all_pos_samples) >= num_pos_samples:\n",
    "            pos_samples = random.sample(all_pos_samples, num_pos_samples)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            p_remaining = num_pos_samples - len(all_pos_samples)\n",
    "#             print(p_remaining, num_pos_samples, len(all_pos_samples))\n",
    "            for _ in range(p_remaining):\n",
    "                all_pos_samples.append(random.choice(all_pos_samples))\n",
    "            \n",
    "            pos_samples = all_pos_samples\n",
    "            \n",
    "        \n",
    "        \n",
    "        if len(all_neg_samples) >= num_neg_samples:\n",
    "            neg_samples = random.sample(all_neg_samples, num_neg_samples)\n",
    "            \n",
    "        else:\n",
    "\n",
    "            n_remaining = num_neg_samples - len(all_neg_samples)\n",
    "            for _ in range(n_remaining):\n",
    "                all_neg_samples.append(random.choice(all_neg_samples))\n",
    "            \n",
    "            neg_samples = all_neg_samples\n",
    "            \n",
    "        return pos_samples, neg_samples\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    ################################\n",
    "    \n",
    "    def get_new_bbox_values_wrt_action(self, action_index, bbox_values):\n",
    "        \n",
    "        '''\n",
    "        If action is not stop or scaleup or scaledown,\n",
    "        add delta_x and delta_y to the first two values of self.ACTIONS[action_index]  [(1, 0), (-1, 0), (0, 1), (0, -1)\n",
    "        (2, 0), (-2, 0), (0, 2), (-2, 0)] and add the values to the old x and y\n",
    "        \n",
    "        if action is stop, then return the bbox values as it is.\n",
    "        \n",
    "        if action is scale_up or scale_down then multiply delta_x and delta_y to 3, 4 values of self.ACTIONS[action_index] and add the\n",
    "        new values to the old width and height\n",
    "        '''\n",
    "#         print(bbox_values, action_index)\n",
    "        \n",
    "        delta_x = self.alpha * bbox_values[2] # 0.03 * width\n",
    "        delta_y = self.alpha * bbox_values[3] # 0.03 * height\n",
    "            \n",
    "        if action_index < 8:\n",
    "            delta_x = max(1, delta_x)\n",
    "            delta_y = max(1, delta_y)\n",
    "            \n",
    "            new_delta_x = self.ACTIONS[action_index][0] * delta_x\n",
    "            new_delta_y = self.ACTIONS[action_index][1] * delta_y\n",
    "            \n",
    "            new_x = int(round(bbox_values[0] + new_delta_x))\n",
    "            new_y = int(round(bbox_values[1] + new_delta_y))\n",
    "            \n",
    "            return [new_x, new_y, bbox_values[2], bbox_values[3]]\n",
    "        \n",
    "        elif action_index == 8:\n",
    "            return bbox_values\n",
    "        \n",
    "        else:\n",
    "            delta_x = max(2, delta_x)\n",
    "            delta_y = max(2, delta_y)\n",
    "            \n",
    "            new_delta_w = self.ACTIONS[action_index][2] * delta_x\n",
    "            new_delta_h = self.ACTIONS[action_index][3] * delta_y\n",
    "            \n",
    "            new_w = int(round(new_delta_w + bbox_values[2]))\n",
    "            new_h = int(round(new_delta_h + bbox_values[3]))\n",
    "            \n",
    "            return [bbox_values[0], bbox_values[1], new_w, new_h]\n",
    "            \n",
    "    \n",
    "    \n",
    "    ################################\n",
    "    \n",
    "    def get_action_and_class_label_for_noisy_samples(self, gt_values, noisy_samples, img_path, samples_types):\n",
    "        bbox_action_class_label_list = []\n",
    "        if samples_type == 'pos:'\n",
    "            for noisy_box_value in noisy_samples:\n",
    "\n",
    "                iou_of_new_values = []\n",
    "                new_gt_values_after_action = []\n",
    "                for index in range(self.NUM_ACTIONS):\n",
    "\n",
    "                    new_values = self.get_new_bbox_values_wrt_action(index, noisy_box_value)\n",
    "        #             print( ' ' + str(new_values))\n",
    "                    iou = self.calculate_IOU(gt_values, new_values)\n",
    "                    iou_of_new_values.append(iou)\n",
    "                    new_gt_values_after_action.append(new_values)\n",
    "\n",
    "                action_label = self.get_max_iou_index(iou_of_new_values) # index of the iou with maximum value which corresponds to the action performed.\n",
    "                class_label = self.get_class_label(gt_values, new_gt_values_after_action[action_label])\n",
    "                bbox_action_class_label_list.append(np.concatenate([noisy_box_value, [action_label], [class_label], [img_path]]))\n",
    "\n",
    "            return bbox_action_class_label_list\n",
    "        else:\n",
    "            for box in noisy_samples:\n",
    "                action_label = -1\n",
    "                class_label = 1\n",
    "                bbox_action_class_label_list.append(np.concatenate([box], [action_label], [class_label], [img_path]))\n",
    "            return bbox_action_class_label_list\n",
    "    \n",
    "    \n",
    "    \n",
    "    ##################################\n",
    "    \n",
    "    def get_noisy_samples_action_and_class_label_for_single_gt_value(self, gt_values, img_path, num_of_samples, pos_threshold, neg_threshold):\n",
    "        \n",
    "    \n",
    "        pos_samples, neg_samples = self.get_pos_neg_noisy_boxes(gt_values, img_path, num_of_samples, pos_threshold, neg_threshold)\n",
    "        pos_samples_action_class_labels_list = self.get_action_and_class_label_for_noisy_samples(gt_values, pos_samples, img_path, 'pos')\n",
    "        neg_samples_action_class_labels_list = self.get_action_and_class_label_for_noisy_samples(gt_values, neg_samples, img_path, 'neg')\n",
    "        \n",
    "        \n",
    "        return pos_samples_action_class_labels_list, neg_samples_action_class_labels_list\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ################################\n",
    "    \n",
    "    def get_class_label(self, gt_values, max_iou_bbox_value):\n",
    "        iou = self.calculate_IOU(gt_values, max_iou_bbox_value)\n",
    "        if iou > 0.7:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    ################################\n",
    "        \n",
    "    def create_new_dir(self, new_path):\n",
    "        if not os.path.exists(new_path):\n",
    "            os.makedirs(new_path)\n",
    "    \n",
    "    \n",
    "    \n",
    "    ################################\n",
    "    \n",
    "    def save_new_values_into_a_txt_file(self, values_list, txt_file_path):\n",
    "        '''\n",
    "        This function reads in every entry and then joins all the values in a single entry using a comma and then write that row as one entity\n",
    "        '''\n",
    "        with open(txt_file_path, \"w\") as output:\n",
    "            for row in values_list:\n",
    "                row = ','.join(map(str, row)) \n",
    "                output.write(row + '\\n')\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    ################################\n",
    "\n",
    "    def get_max_iou_index(self, iou_for_all_actions):\n",
    "        \n",
    "        max_index = np.argmax(iou_for_all_actions)\n",
    "        \n",
    "        return max_index\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     ################################\n",
    "    \n",
    "#     def get_action_and_class_label_for_single_gt_value(self, gt_values, img_path, class_name):\n",
    "        \n",
    "#         bbox_action_class_label_list = self.perform_actions_and_get_IOU(gt_values, img_path, class_name)\n",
    "        \n",
    "#         return bbox_action_class_label_list\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    ################################\n",
    "    \n",
    "    def generate_train_data_wrt_each_gt_value(self, frames_path, gt_values_path, class_name, save_to_txt_file):\n",
    "        '''\n",
    "        action_class_bbox.txt file values\n",
    "        x, y, w, h, action_label, class_label, img_path\n",
    "        '''\n",
    "        all_pos_samples = []  # action_label, class_label, bbox_values with max iou\n",
    "        all_neg_samples = []\n",
    "        all_imgs_names = os.listdir(frames_path)\n",
    "        all_imgs_names.sort(key=lambda f: int(re.sub('\\D', '', f))) # sort wrt to the image name '001.jpg', '002.jpg', ...\n",
    "        all_images_full_path = []\n",
    "#         new_path = self.GENERATED_DATA_PATH + class_name + '/img/'\n",
    "#         self.create_dir_for_new_noisy_imgs_for_each_class(new_path)\n",
    "        \n",
    "        gt_values = self.get_gt_values(gt_values_path)\n",
    "        \n",
    "        if len(gt_values) == len(all_imgs_names):\n",
    "            for index, image_name in enumerate(all_imgs_names):\n",
    "                \n",
    "#                 noisy_image = self.add_gaussian_noise(frames_path + image_name)\n",
    "#                 Image.fromarray(noisy_image).save(new_path + image_name)\n",
    "                if '.jpg' in image_name or '.png' in image_name or '.jpeg' in image_name:\n",
    "                    img_path = frames_path + image_name\n",
    "#                     bbox_action_class_label_list = self.get_action_and_class_label_for_single_gt_value(gt_values[index], img_path, class_name, num_of_samples) \n",
    "    #                 bbox_action_class_label_list.append([index]) \n",
    "                    pos_samples_list, neg_samples_list = self.get_noisy_samples_action_and_class_label_for_single_gt_value(gt_values[index], img_path, 250, \n",
    "                                                                                                                          config.SL_POS_SAMPS_THRESHOLD,\n",
    "                                                                                                                          config.SL_NEG_SAMPS_THRESHOLD)\n",
    "                                                                                                                 \n",
    "#                     bbox_action_class_values = bbox_action_class_values + bbox_action_class_label_list.copy()\n",
    "                    all_pos_samples = all_pos_samples + pos_samples_list.copy()\n",
    "                    all_neg_samples = all_neg_samples + neg_samples_list.copy()\n",
    "\n",
    "                    if index >= 5:\n",
    "                        break\n",
    "            \n",
    "            \n",
    "            if save_to_txt_file:\n",
    "                dir_path_to_save_txt_files = self.GENERATED_DATA_PATH + class_name + '/'\n",
    "                self.create_new_dir(dir_path_to_save_txt_files)\n",
    "#                 self.save_new_values_into_a_txt_file(bbox_action_class_values, dir_path_to_save_txt_files + '_action_class_bbox.txt')\n",
    "                self.save_new_values_into_a_txt_file(all_pos_samples, dir_path_to_save_txt_files + 'pos_action_class_bbox.txt')\n",
    "                self.save_new_values_into_a_txt_file(all_neg_samples, dir_path_to_save_txt_files + 'neg_action_class_bbox.txt')\n",
    "            \n",
    "            else:\n",
    "                return all_pos_samples, all_neg_samples\n",
    "#             self.save_new_values_into_a_txt_file(all_images_full_path, dir_path_to_save_txt_files + 'images_names_index.txt')\n",
    "     \n",
    "    \n",
    "    \n",
    "    ################################\n",
    "    \n",
    "    def get_train_samples(self):\n",
    "        '''\n",
    "        For the current dataset, Data directory contains directories with class names which has images and correspoding one \n",
    "        ground truth values txt file\n",
    "        '''\n",
    "        all_dirs_in_data = os.listdir(self.TRAIN_DATA_PATH)\n",
    "        for class_name in all_dirs_in_data:\n",
    "#             if class_name == 'Biker':\n",
    "            frames_data_path = self.TRAIN_DATA_PATH + class_name + '/img/'\n",
    "            gt_values_path  = self.TRAIN_DATA_PATH + class_name + '/groundtruth_rect.txt'\n",
    "\n",
    "            self.generate_train_data_wrt_each_gt_value(frames_data_path, gt_values_path, class_name, True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_samples_obj = GENERATE_SAMPLES()\n",
    "generate_samples_obj.get_train_samples()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112, 112, 3)\n"
     ]
    }
   ],
   "source": [
    "adnet = ADNetwork()\n",
    "adnet_model = adnet.create_network()\n",
    "m = adnet_model\n",
    "input_data = np.arange(1 * 112 * 112 * 3).reshape(1, 112, 112, 3)\n",
    "print(np.shape(input_data[0]))\n",
    "action_hist = np.zeros(shape = (1, 1, 1, 110))\n",
    "\n",
    "action_logits, class_logits = m([input_data, action_hist], training = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.54080665>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.nn.softmax(class_logits)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_43\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_43 (InputLayer)           [(None, 112, 112, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv_1 (Conv2D)                 (None, 53, 53, 96)   14208       input_43[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_LRN_63 (TensorFlowO [(None, 53, 53, 96)] 0           conv_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_63 (MaxPooling2D) (None, 52, 52, 96)   0           tf_op_layer_LRN_63[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv_2 (Conv2D)                 (None, 24, 24, 256)  614656      max_pooling2d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_LRN_64 (TensorFlowO [(None, 24, 24, 256) 0           conv_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_64 (MaxPooling2D) (None, 11, 11, 256)  0           tf_op_layer_LRN_64[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv_3 (Conv2D)                 (None, 5, 5, 512)    1180160     max_pooling2d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_LRN_65 (TensorFlowO [(None, 5, 5, 512)]  0           conv_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_65 (MaxPooling2D) (None, 3, 3, 512)    0           tf_op_layer_LRN_65[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "fc_4 (Conv2D)                   (None, 1, 1, 512)    2359808     max_pooling2d_65[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "fc4_dropout (Dropout)           (None, 1, 1, 512)    0           fc_4[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "input_44 (InputLayer)           [(None, 1, 1, 110)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_21 (Concatenate)    (None, 1, 1, 622)    0           fc4_dropout[0][0]                \n",
      "                                                                 input_44[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "fc_5 (Conv2D)                   (None, 1, 1, 512)    318976      concatenate_21[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "fc5_dropout (Dropout)           (None, 1, 1, 512)    0           fc_5[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "action (Conv2D)                 (None, 1, 1, 11)     5643        fc5_dropout[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "class (Conv2D)                  (None, 1, 1, 2)      1026        fc5_dropout[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_42 (Flatten)            (None, 11)           0           action[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_43 (Flatten)            (None, 2)            0           class[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 4,494,477\n",
      "Trainable params: 4,494,477\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv_1/kernel:0\n",
      "conv_1/bias:0\n",
      "conv_2/kernel:0\n",
      "conv_2/bias:0\n",
      "conv_3/kernel:0\n",
      "conv_3/bias:0\n",
      "fc_4/kernel:0\n",
      "fc_4/bias:0\n",
      "fc_5/kernel:0\n",
      "fc_5/bias:0\n",
      "action/kernel:0\n",
      "action/bias:0\n",
      "class/kernel:0\n",
      "class/bias:0\n"
     ]
    }
   ],
   "source": [
    "for variable in adnet_model.trainable_variables:\n",
    "    print(variable.name)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "###############################################\n",
    "############# PRETRAINING DATALOADING #########\n",
    "###############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ADNet_pretraining_data_loading:\n",
    "    def __init__(self):\n",
    "        self.GENERATED_DATA_PATH = config.GENERATED_DATA_PATH\n",
    "        self.bbox_action_class_img_path_list = []\n",
    "        self.pos_bbox_action_class_path_list = []\n",
    "        self.neg_bbox_action_class_path_list = []\n",
    "        self.NUM_ACTIONS = 11\n",
    "        self.NUM_CLASSES = 2\n",
    "#         self.read_generated_txt_files()\n",
    "        self.get_all_created_dirs()\n",
    "    \n",
    "    def get_all_created_dirs(self):\n",
    "        self.generated_dirs = os.listdir(self.GENERATED_DATA_PATH)\n",
    "        \n",
    "    \n",
    "    \n",
    "    def get_values_from_txt_files(self, bbox_action_label_txt_path):\n",
    "        \n",
    "#         bbox_action_label_txt_path = '/Users/vijay/Downloads/Code_Data/ADNet/generated_data/Biker/action_class_bbox.txt'\n",
    "        bbox_action_class_img_path_list = []\n",
    "#         self.pos_bbox_action_class_path_list = []\n",
    "#         self.neg_bbox_action_class_path_list = []\n",
    "        with open(bbox_action_label_txt_path, 'r') as f:\n",
    "\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "\n",
    "                if not line.strip():\n",
    "                    continue\n",
    "                line_split = line.split(',')\n",
    "#                 x, y, w, h, action, class_label, img_path = [int(x.strip()) if index < 6 else x.strip() for index, x in enumerate(line_split)]\n",
    "                x, y, w, h, action, class_label, img_path = [int(x.strip()) if index < 6 else x.strip() for index, x in enumerate(line_split)]\n",
    "                bbox_action_class_img_path_list.extend([[[x, y, w, h], action, class_label, img_path]])\n",
    "        \n",
    "        if 'pos' in bbox_action_label_txt_path:\n",
    "            \n",
    "            self.pos_bbox_action_class_path_list = bbox_action_class_img_path_list\n",
    "        else:\n",
    "            \n",
    "            self.neg_bbox_action_class_path_list = bbox_action_class_img_path_list\n",
    "        \n",
    "        \n",
    "    \n",
    "    def read_generated_txt_files(self, path):\n",
    "        class_dirs = glob.glob(path + '*')\n",
    "        self.bbox_action_class_img_path_list = []\n",
    "        for text_file_path in class_dirs:\n",
    "#             print(text_file_path)\n",
    "            self.get_values_from_txt_files(text_file_path)\n",
    "        \n",
    "    \n",
    "    @tf.function\n",
    "    def parse_data(self, row_data):\n",
    "        batch_data = []\n",
    "#         np_row_data = row_data.numpy()\n",
    "        for entry in row_data:\n",
    "            \n",
    "#             np_entry = entry.numpy()\n",
    "            sub_list = []\n",
    "            for index, item in enumerate(entry):\n",
    "                \n",
    "                if index != 6:\n",
    "                    item = int(tf.compat.as_str_any(item.numpy()))\n",
    "                \n",
    "                else:\n",
    "                    item = tf.compat.as_str_any(item)\n",
    "                sub_list.append(item)\n",
    "\n",
    "            batch_data.append(sub_list)\n",
    "        return batch_data\n",
    "                    \n",
    "    \n",
    "    \n",
    "    def get_input_and_labels_for_pretraining_adnet(self, batch_data):\n",
    "        input_bbox_values = []\n",
    "        action_labels = []\n",
    "        class_labels = []\n",
    "        img_paths = []\n",
    "        for input_label_path in batch_data:\n",
    "            bbox_values = []\n",
    "            for index, value in enumerate(input_label_path):\n",
    "                if index < 4:\n",
    "                    bbox_values.append(values)\n",
    "                elif index == 4:\n",
    "                    action_labels.append(value)\n",
    "                elif index == 5:\n",
    "                    class_labels.append(value)\n",
    "                else:\n",
    "                    img_paths.append(value)\n",
    "            input_bbox_values.append(bbox_values)\n",
    "        return input_bbox_values\n",
    "    \n",
    "    \n",
    "    \n",
    "    def convert_labels_to_one_hot(self, label, label_type = 'action'):\n",
    "#         one_hot_labels = []\n",
    "        if label_type == 'action':\n",
    "            zeros_length = self.NUM_ACTIONS\n",
    "        elif label_type == 'class':\n",
    "            zeros_length = self.NUM_CLASSES\n",
    "            \n",
    "        zero_label = np.zeros(shape = [1, zeros_length], dtype = np.float16)\n",
    "        if label == -1:\n",
    "            return zero_label\n",
    "        else:\n",
    "            \n",
    "            zero_label[0,label] = 1\n",
    "            return zero_label\n",
    "        \n",
    "    \n",
    "    \n",
    "    def get_image_using_bbox_values(self, bbox_values, img_path):\n",
    "#         print(bbox_values, img_path)\n",
    "        img = cv2.imread(img_path)\n",
    "#         print(np.shape(img))\n",
    "        x, y = bbox_values[0], bbox_values[1]\n",
    "        xmax, ymax = x + bbox_values[2], y + bbox_values[3]\n",
    "        \n",
    "        cropped_img = img[y : ymax, x : xmax]\n",
    "        resized_img = cv2.resize(cropped_img, (112, 112))\n",
    "#         return np.expand_dims(resized_img, axis = 0)\n",
    "        return resized_img\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def get_dataset(self):\n",
    "\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(self.bbox_action_class_img_path_list)\n",
    "\n",
    "        dataset = dataset.shuffle(buffer_size = 100)\n",
    "        dataset = dataset.batch(5)\n",
    "        dataset = dataset.map(lambda row :self.parse_data(row)) \n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adnet_pretraining_data = ADNet_pretraining_data_loading()\n",
    "\n",
    "# adnet_pretraining_data.read_generated_txt_files('/Users/vijay/Downloads/Code_Data/ADNet/generated_data/Biker/pos_action_class_bbox.txt')\n",
    "# # dataset_getter = adnet_pretraining_data.get_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "###############################################\n",
    "############# AD NETWORK ######################\n",
    "###############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ADNetwork:\n",
    "    def __init__(self):\n",
    "        self.NUM_ACTIONS = 11 # LEFT, RIGHT, UP, DOWN, DOUBLE_LEFT, DOUBLE_RIGHT, DOUBLE_UP, DOUBLE_DOWN, SCLAE_UP, SCALE_DOWN, STOP\n",
    "        self.NUM_CLASSES = 2 # OBJECT, BACKGROUND\n",
    "        self.REQ_ACTION_HISTORY = 10 # Last 10 values of actions are required\n",
    "       \n",
    "        \n",
    "#         self.action_history = np.zeros(shape=(1, 110)).astype(np.float32)\n",
    "        \n",
    "    \n",
    "        \n",
    "    \n",
    "    def create_network(self):\n",
    "        input_data = Input(shape = (112, 112, 3))\n",
    "        action_history = Input(shape = (1, 1, 110))\n",
    "        \n",
    "        \n",
    "        ## CONV_1\n",
    "#         print(np.shape(input_data), np.shape(action_history))\n",
    "        data = Conv2D(filters = 96, kernel_size = (7, 7), strides = (2, 2), padding = 'VALID', activation = 'relu', name = 'conv_1')(input_data)\n",
    "#         data = LayerNormalization()(data)\n",
    "#         print('conv_1 ' + str(np.shape(data)) + ' ' + str(np.shape(action_history)))\n",
    "        data = tf.nn.local_response_normalization(data, depth_radius = 5, bias = 2, alpha = 1e-4*5, beta = 0.75)\n",
    "        data = MaxPool2D(pool_size=(2, 2), strides = (1, 1), padding = 'VALID')(data)\n",
    "\n",
    "\n",
    "        ## CONV_2\n",
    "        data = Conv2D(filters = 256, kernel_size = (5, 5), strides = (2, 2), padding = 'VALID', activation = 'relu', name = 'conv_2')(data)\n",
    "#         data = LayerNormalization()(data)\n",
    "        data = tf.nn.local_response_normalization(data, depth_radius = 5, bias = 2, alpha = 1e-4*5, beta = 0.75)\n",
    "        data = MaxPool2D(pool_size=(3, 3), strides = (2, 2), padding = 'VALID')(data)\n",
    "\n",
    "\n",
    "        ## CONV_3\n",
    "        data = Conv2D(filters = 512, kernel_size = (3, 3), strides = (2, 2), padding = 'VALID', activation = 'relu', name = 'conv_3')(data)\n",
    "#         data = LayerNormalization()(data)\n",
    "        data = tf.nn.local_response_normalization(data, depth_radius = 5, bias = 2, alpha = 1e-4*5, beta = 0.75)\n",
    "        data = MaxPool2D(pool_size=(3, 3), strides = (1, 1), padding = 'VALID')(data)\n",
    "\n",
    "    \n",
    "        data = Conv2D(filters = 512, kernel_size = (3, 3), strides = (1, 1), padding = 'VALID', activation = 'relu', name='fc_4')(data)\n",
    "        data = Dropout(rate = 0.5, name = 'fc4_dropout')(data)\n",
    "        data = Concatenate(axis = -1)([data, action_history])\n",
    "#         data = Concatenate(axis = -1)([])\n",
    "        data = Conv2D(filters = 512, kernel_size = [1, 1], strides = (1, 1), padding='VALID', activation = 'relu', name = 'fc_5')(data)\n",
    "                               \n",
    "        data = Dropout(rate = 0.5, name = 'fc5_dropout')(data)\n",
    "\n",
    "        # auxilaries\n",
    "        action_logits = Conv2D(filters = 11, kernel_size = [1, 1], strides = (1, 1), padding='VALID', name = 'action')(data)\n",
    "        class_logits = Conv2D(filters = 2, kernel_size = [1, 1], strides = (1, 1), padding='VALID', name = 'class')(data)\n",
    "        action_logits_f = Flatten()(action_logits)\n",
    "        class_logits_f = Flatten()(class_logits)\n",
    "        \n",
    "        \n",
    "        return Model([input_data, action_history], [action_logits_f, class_logits_f])\n",
    "        \n",
    "    \n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "###############################################\n",
    "############# ALL TRAININGS ###################\n",
    "###############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train_networks:\n",
    "    def __init__(self):\n",
    "        self.adnet_ckpt_dir = config.ADNET_CKPT_DIR\n",
    "        self.FINAL_WEIGHTS_DIR = config.FINAL_WEIGHTS_DIR\n",
    "        adnetwork = ADNetwork()\n",
    "        \n",
    "        adent_model = adnetwork.create_network()\n",
    "        self.adnet_ckpt = tf.train.Checkpoint(curr_epoch = tf.Variable(0),\n",
    "                                                        optimizer_1 = SGD(learning_rate = 0.0001, momentum = 0.9),\n",
    "                                                        optimizer_2 = SGD(learning_rate = 0.001, momentum = 0.9),\n",
    "                                                         model = adnet_model\n",
    "                                                 )\n",
    "        self.adnet_ckpt_manager = tf.train.CheckpointManager(self.adnet_ckpt,\n",
    "                                                                directory = self.adnet_ckpt_dir,\n",
    "                                                                max_to_keep = 3)\n",
    "        self.adnet_pretrain_action_history = np.zeros(shape = (config.SL_PRETRAIN_BATCH_SIZE, 1, 1, 110), dtype = np.float16)\n",
    "        self.adnet_model_ip = []\n",
    "#         self.act_hist = np.zeros(config.SL_PRETRAIN_BATCH_SIZE * 1 * 1* 110).reshape(config.SL_PRETRAIN_BATCH_SIZE, 1, 1, 110)\n",
    "        self.generate_samples_obj = GENERATE_SAMPLES()\n",
    "        self.adnet_pretrain_data_loader = ADNet_pretraining_data_loading()\n",
    "        self.adnet_action_labels = []\n",
    "        self.adnet_class_labels = []\n",
    "       \n",
    "    \n",
    "    '''###################################'''\n",
    "    \n",
    "    def create_dirs(self):\n",
    "        \n",
    "        if not os.path.exists(config.GENERATED_DATA_PATH):\n",
    "            os.makedirs(config.GENERATED_DATA_PATH)\n",
    "\n",
    "        if not os.path.exists(config.FINAL_WEIGHTS_DIR):\n",
    "            os.makedirs(config.FINAL_WEIGHTS_DIR)\n",
    "    \n",
    "    \n",
    "    \n",
    "    '''###################################'''\n",
    "    \n",
    "    def restore_recent_checkpoint(self):\n",
    "        \n",
    "        if self.adnet_ckpt_manager.latest_checkpoint:\n",
    "            self.adnet_ckpt.restore(self.adnet_ckpt_manager.latest_checkpoint)\n",
    "    \n",
    "    \n",
    "    \n",
    "    '''###################################'''\n",
    "    \n",
    "    def update_action_label_variables(self, input_batch, action_labels, action_history):\n",
    "        with tf.GradientTape(persistent = True) as action_tape:\n",
    "            action_logits, _ = self.adnet_ckpt.model([input_batch, action_history], training = True)\n",
    "            action_vars = []\n",
    "            \n",
    "            for variable in self.adnet_ckpt.model.trainable_variables:\n",
    "                if 'action' in variable.name or 'fc' in variable.name:\n",
    "                    action_vars.append(variable)\n",
    "            \n",
    "            action_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = action_labels, logits = action_logits))\n",
    "        \n",
    "        action_gradients = action_tape.gradient(action_loss, action_vars)\n",
    "        action_grad_ops = self.adnet_ckpt.optmizer_1.apply_gradients(zip(action_gradients, action_vars))\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    '''###################################'''\n",
    "    \n",
    "    def update_class_label_variables(self, input_batch, class_labels, action_history):\n",
    "        with tf.GradientTape(persistent = True) as class_tape:\n",
    "            _, class_logits = self.adnet_ckpt.model([input_batch, action_history], training = True)\n",
    "            class_vars = []\n",
    "            \n",
    "            for variable in self.adnet_ckpt.model.trainable_variables:\n",
    "                if 'class' in variable.name or 'fc' in variable.name:\n",
    "                    class_vars.append(variable)\n",
    "            \n",
    "            class_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = class_labels, logits = class_logits))\n",
    "        \n",
    "        class_gradients = class_tape.gradient(class_loss, class_vars)\n",
    "        class_grad_ops = self.adnet_ckpt.optmizer_2.apply_gradients(zip(class_gradients, class_vars))\n",
    "    \n",
    "    \n",
    "     \n",
    "   \n",
    "    '''###################################'''\n",
    "    \n",
    "    def get_batch_imgs_action_and_class_labels(self, batch_data):\n",
    "        imgs = []\n",
    "        action_labels = []\n",
    "        class_labels = []\n",
    "        for values in batch_data: # values is a list item\n",
    "            imgs.append(self.adnet_pretrain_data_loader.get_image_using_bbox_values(values[0], values[-1]))\n",
    "            \n",
    "            action_label = self.adnet_pretrain_data_loader.convert_labels_to_one_hot(values[1], 'action')\n",
    "            class_label = self.adnet_pretrain_data_loader.convert_labels_to_one_hot(values[2], 'class')\n",
    "\n",
    "            action_labels.append(action_label)\n",
    "            class_labels.append(class_label)\n",
    "#         print(np.shape(imgs))\n",
    "        return imgs, action_labels, class_labels\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "########################################\n",
    "##### PRETRAINING USING SVL ############\n",
    "########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pretrain_SL(Train_networks):\n",
    "    def __init__(self):\n",
    "        super.__init__()\n",
    "        \n",
    "    \n",
    "    \n",
    "    '''###################################'''\n",
    "    \n",
    "    def in_each_pretrain_adnet_step(self, imgs, action_labels, class_labels, train_mode = 'both', action_history = None):\n",
    "        \n",
    "#         test = True\n",
    "        if train_mode == 'action':\n",
    "            self.update_action_label_variables(imgs, action_labels, action_history)\n",
    "        \n",
    "        elif train_mode == 'class':\n",
    "            self.update_class_label_variables(imgs, class_labels, action_history)\n",
    "        \n",
    "        else:\n",
    "            p_imgs = imgs[0]\n",
    "            n_imgs = imgs[1]\n",
    "            cls_imgs = p_imgs + n_imgs\n",
    "            \n",
    "            p_action_labels = action_labels[0]\n",
    "            \n",
    "            p_class_labels = class_labels[0]\n",
    "            n_class_labels = class_labels[1]\n",
    "            \n",
    "            cls_class_labels = p_class_labels + n_class_labels\n",
    "            \n",
    "            '''Pretraining ADNetwork with supervised learning'''\n",
    "            with tf.GradientTape(persistent = True) as conv_tape, tf.GradientTape(persistent = True) as fc_tape:\n",
    "#                 print(np.shape(self.adnet_model_ip))\n",
    "\n",
    "                action_logits, _ = self.adnet_ckpt.model([p_imgs, self.adnet_pretrain_action_history], training = True)\n",
    "                _, class_logis   = self.adnet_ckpt.model([cls_imgs, self.adnet_pretrain_action_history], training = True)\n",
    "\n",
    "                conv_vars = []\n",
    "                fc_vars = []\n",
    "                \n",
    "                for variable in self.adnet_ckpt.model.trainable_variables:\n",
    "                    if 'action' not in variable.name and 'class' not in variable.name:\n",
    "                        conv_vars.append(variable)\n",
    "                    else:\n",
    "                        fc_vars.append(variable)\n",
    "\n",
    "            \n",
    "                action_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = p_action_labels, logits  = action_logits))\n",
    "                class_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = cls_class_labels, logits = class_logits))\n",
    "                total_loss = action_loss + class_loss\n",
    "            \n",
    "            print(total_loss.numpy())\n",
    "            conv_gradients = conv_tape.gradient(total_loss, conv_vars)\n",
    "            fc_gradients = fc_tape.gradient(total_loss, fc_vars)\n",
    "\n",
    "            conv_grad_ops = self.adnet_ckpt.optimizer_1.apply_gradients(zip(conv_gradients, conv_vars))\n",
    "            fc_grad_ops = self.adnet_ckpt.optimizer_2.apply_gradients(zip(fc_gradients, fc_vars))\n",
    "        \n",
    "\n",
    "            return total_loss\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    '''###################################'''\n",
    "    \n",
    "    def pretrain_adnet_SL(self):\n",
    "                                                                     \n",
    "        loss_log = tf.keras.metrics.Mean('loss', dtype = tf.float32)\n",
    "                                                  \n",
    "        action_history = np.zeros(shape=(1, 1, 1, 110)) # during adnet pretraining, action history values vector is set to 0\n",
    "#         adnetwork = ADNetwork()\n",
    "#         self.adnet_pretrain_model = adnetwork.create_network()\n",
    "        adnet_pretrain_data_loader = ADNet_pretraining_data_loading()\n",
    "        self.restore_recent_checkpoint()\n",
    "        \n",
    "        '''\n",
    "        generated_dirs contains folders with a txt file containing bbox values, action_label, class label and img_path of generated files\n",
    "        We read each text file, and train the network for 300 epochs for each txt file\n",
    "        '''\n",
    "        for label_dir in self.adnet_pretrain_data_loader.generated_dirs:\n",
    "            '''before reading a new set of gen text file (pos and neg), clear the respective pos and neg lists accumulated with\n",
    "            bbox and label values of previously read label dir files'''\n",
    "            self.adnet_pretrain_data_loader.pos_bbox_action_class_path_list.clear()\n",
    "            self.adnet_pretrain_data_loader.neg_bbox_action_class_path_list.clear()\n",
    "            txt_files_path = self.adnet_pretrain_data_loader.GENERATED_DATA_PATH + label_dir + '/'\n",
    "            if '.DS_Store' not in txt_files_path:\n",
    "                self.adnet_pretrain_data_loader.read_generated_txt_files(txt_files_path)\n",
    "\n",
    "                num_of_pos_samples = int(config.SL_PRETRAIN_BATCH_SIZE * 0.5)\n",
    "                num_of_neg_samples = config.SL_PRETRAIN_BATCH_SIZE - num_of_pos_samples\n",
    "\n",
    "                for epoch in range(config.SL_PRETRAIN_EPOCHS):\n",
    "\n",
    "#                     print(len(adnet_pretrain_data_loader.pos_bbox_action_class_path_list), len(adnet_pretrain_data_loader.neg_bbox_action_class_path_list))\n",
    "                    pos_samples = random.sample(self.adnet_pretrain_data_loader.pos_bbox_action_class_path_list, num_of_pos_samples)\n",
    "                    neg_samples = random.sample(self.adnet_pretrain_data_loader.neg_bbox_action_class_path_list, num_of_neg_samples)\n",
    "\n",
    "\n",
    "                    p_imgs, p_action_labels, p_class_labels = self.get_batch_imgs_action_and_class_labels(pos_samples)\n",
    "                    n_imgs, n_action_labels, n_class_labels = self.get_batch_imgs_action_and_class_labels(neg_samples)\n",
    "\n",
    "                    '''\n",
    "                    To train the model to predict class scores, we use both pos and neg samples. pos samples, since their IOU is greater than 0.7,\n",
    "                    class label value would be 1 for all those positive samples. And for neg samples, the same class score label would be 0\n",
    "                    '''\n",
    "                    cls_imgs = p_imgs + n_imgs\n",
    "                    cls_class_labels = [1] * len(p_imgs) + [0] * len(n_imgs)\n",
    "                    \n",
    "                    total_loss = self.in_each_pretrain_adnet_step([p_imgs, n_imgs], [p_action_labels, None], [p_class_labels, n_class_labels], 'both')\n",
    "                    loss_log.update_state(total_loss)\n",
    "\n",
    "                    if epoch % 100 == 0 and epoch > 0: \n",
    "                        self.adnet_pretrain_ckpt_manager.save()\n",
    "\n",
    "                    if epoch == config.SL_PRETRAIN_EPOCHS -1:\n",
    "                        self.adnet_ckpt.model.save_weights(config.FINAL_WEIGHTS_DIR + 'adnet_SL_pretrain_weights.h5')\n",
    "                        print('ADNet model SL pre-training finished')\n",
    "\n",
    "\n",
    "                with open(config.LABEL_DIRS_TRAINED_TXT_PATH, \"a\") as output:\n",
    "                    for row in [label_dir, self.adnet_pretraining_data_loader.GENERATED_DATA_PATH + label_dir + '/']:\n",
    "                        row = ','.join(map(str, row)) \n",
    "                        output.write(row + '\\n')\n",
    "    #             print('loss is ' + str(loss_log.result()))\n",
    "                loss_log.reset_states()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 1, 2) (6, 4, 5)\n"
     ]
    }
   ],
   "source": [
    "l = [1, 2, 3]\n",
    "k = [4, 5, 6]\n",
    "j = list(zip(l, k))\n",
    "random.shuffle(j)\n",
    "l, k = zip(*j)\n",
    "print(l, k)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "########################################\n",
    "##### PRETRAINING USING RL ############\n",
    "########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pretrain_RL(Train_networks):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    '''###################################'''\n",
    "    \n",
    "    def compute_z_score(self, gt_bbox_values, gen_bbox_values):\n",
    "        z_scores = []\n",
    "        if len(gt_bbox_values) == len(gen_bbox_values):\n",
    "            for gt_bbox, gen_bbox in zip(gt_bbox_values, gen_bbox_values):\n",
    "                iou = self.generate_samples_obj.calculate_IOU(gt_bbox, gen_bbox)\n",
    "                if iou > 0.7:\n",
    "                    z_scores.append(1)\n",
    "                else:\n",
    "                    z_scores.append(0)\n",
    "        \n",
    "        return z_scores\n",
    "    \n",
    "    \n",
    "\n",
    "    '''###################################'''\n",
    "    \n",
    "    def initial_finetune_adnet_pretrain_RL(self):\n",
    "        '''\n",
    "        finetune for the first two images\n",
    "        When we are training only using action_label values, then it doesn't make any sense in using negative samples during training as\n",
    "        the IOU of negative samples is less than 0.3\n",
    "        '''\n",
    "        pos_samples, _ = self.generate_samples_obj.get_noisy_samples_action_and_class_label_for_single_gt_value(curr_bbox_values, img_path, 3000, \n",
    "                                                                                                               config.RL_POS_SAMPS_THRESHOLD,\n",
    "                                                                                                               config.RL_NEG_SAMPS_THRESHOLD)\n",
    "        pos_samples_labels = self.prepare_samples_for_training(pos_samples)\n",
    "        p_imgs, p_action_labels, _ = self.get_batch_imgs_action_and_class_labels(pos_samples_labels)\n",
    "        imgs_labels_list = list(zip(p_imgs, p_action_labels))\n",
    "        action_history = np.zeros(config.RL_PRETRAIN_BATCH_SIZE, 1, 1, 110)\n",
    "        for epoch in range(300):\n",
    "            \n",
    "            random.sample(imgs_labels_list, config.RL_PRETRAIN_BATCH_SIZE)\n",
    "            batch_imgs, batch_action_labels = zip(*imgs_labels_list)\n",
    "            self.in_each_pretrain_adnet_step(batch_imgs, batch_action_labels, None, 'action', action_history)\n",
    "        \n",
    "            if epoch % 100 == 0:\n",
    "                self.adnet_ckpt_manager.save()\n",
    "        \n",
    "        \n",
    "        \n",
    "    '''###################################'''\n",
    "    \n",
    "    def tracking_procedure(self, img_path, curr_bbox_values, action_history):\n",
    "        \n",
    "#         action_history = np.zeros(shape = (1, 10, 11), dtype = np.float32)\n",
    "        cropped_img = self.get_img_from_bbox_values(curr_bbox_values, img_path)\n",
    "        \n",
    "    \n",
    "        prev_bbox_values = []\n",
    "#         prev_to_do_action_idx = -1\n",
    "        to_do_action_idx = 0\n",
    "        action_hist_insert_idx = 0\n",
    "#         num_steps_until_termination = 0\n",
    "        boxes_history = []\n",
    "        perform_tracking = True\n",
    "        probs_of_actions = []\n",
    "        boxes_history.append(curr_bbox_values)\n",
    "        prev_action_prob = 0\n",
    "        \n",
    "        '''Tracking simulation has to be stopped if the action selected is STOP or if the agent is oscillating'''\n",
    "        while to_do_action_idx != 8 and perform_tracking:\n",
    "            \n",
    "            \n",
    "            flatten_action_histories = action_history.copy().reshape(1, 1, 1, 110)\n",
    "            \n",
    "            '''get action probabilities'''\n",
    "            action_logits, _ = self.adnet_ckpt.model([np.expand_dims(cropped_img, axis = 0), flatten_action_histories], training = False)\n",
    "            action_probs = tf.nn.softmax(action_logits)\n",
    "            \n",
    "            \n",
    "            to_do_action_idx = np.argmax(action_probs)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            '''\n",
    "            if to_do_action_idx is to STOP then there is no need to perform action on the bbox values. Therefore, it is enough to\n",
    "            return previous curr_bboxes, probability of action selected before termination, action_histories, num_of_steps taken to terminate\n",
    "            '''\n",
    "            if to_do_action_idx != 8: # index 8 = STOP\n",
    "                \n",
    "                prev_action_prob = max(action_probs)\n",
    "                '''get new bbox values'''\n",
    "                temp_bbox_values = self.generate_samples_obj.get_new_bbox_values_wrt_action(to_do_action_idx, curr_bbox_values.copy())\n",
    "\n",
    "                '''\n",
    "                if actions are Left, right, left then it means that agent returned to the earlier place. \n",
    "                If this happens, then we have to terminate the\n",
    "                tracking process. In other words, if a bbox value is repeated more than once, \n",
    "                then we can be certain that oscillation occured.\n",
    "                '''\n",
    "                if temp_bbox_values not in boxes_history:\n",
    "\n",
    "                    boxes.append(temp_bbox_values.copy())\n",
    "\n",
    "                    prev_bbox_values = curr_bbox_values.copy()\n",
    "                    curr_bbox_values = temp_bbox_values.copy()\n",
    "                    \n",
    "\n",
    "\n",
    "\n",
    "                    '''get new img patch using new bbox values'''\n",
    "                    cropped_img = self.generate_samples_obj.get_img_from_bbox_values(curr_bbox_values, img_path)\n",
    "\n",
    "                    '''update the action histories with newly obtained action probs'''\n",
    "                    action_history[:, action_hist_insert_idx, :] = action_probs.copy()\n",
    "                    action_hist_insert_idx = action_hist_insert_idx + 1\n",
    "                    action_hist_insert_idx = 0 if action_hist_insert_idx > 9 else action_hist_insert_idx\n",
    "\n",
    "#                     num_steps_until_termination = num_steps_until_termination + 1\n",
    "\n",
    "\n",
    "\n",
    "                else:\n",
    "                    perfrom_tracking = False\n",
    "\n",
    "            \n",
    "        \n",
    "        return prev_act_prob, action_history, curr_bbox_values\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    '''###################################'''\n",
    "    \n",
    "    def select_a_random_index_for_image_sampling(self, num_of_frames):\n",
    "        random_index = random.choice(np.arange(num_of_frames - config.NUM_FRAMES_IN_PRETRAIN_RL))\n",
    "        return [index+random_index for index in range(10)]\n",
    "        \n",
    "        \n",
    "    \n",
    "    '''###################################'''\n",
    "    \n",
    "    def prepare_samples_for_training(self, batch_data):\n",
    "        samples = []\n",
    "        for sample in batch_data:\n",
    "            temp_samples = []\n",
    "            bbox_values = []\n",
    "#             print(sample)\n",
    "            for index, item in enumerate(sample):\n",
    "                if index < 3:\n",
    "                    bbox_values.append(int(item))\n",
    "                elif index == 3:\n",
    "                    bbox_values.append(int(item))\n",
    "                    temp_samples.append(bbox_values)\n",
    "                elif index > 3 and index < 6:\n",
    "                    temp_samples.append(int(item))\n",
    "                else:\n",
    "                    temp_samples.append(item)\n",
    "                \n",
    "            samples.append(temp_samples)\n",
    "        \n",
    "        \n",
    "        return samples\n",
    "                \n",
    "            \n",
    "    \n",
    "    '''###################################'''\n",
    "    \n",
    "    def perfrom_tracking_and_finetune_weights(self, vid_imgs_path):\n",
    "        '''\n",
    "        vid_imgs_path is a path to a particular dir which contains video frames as images in 'img' dir and correspoinding gt values of the object\n",
    "        that is being tracked in 'groundtruth_rect.txt' file\n",
    "        \n",
    "        Now, randomly select 10 consecutinve images and except for the first image, \n",
    "        perform the tracking and store results in the corresponding lists.\n",
    "        '''\n",
    "        all_imgs_in_dir = os.listdir(vid_imgs_path + 'img/') \n",
    "        gt_values = self.generate_samples_obj.get_gt_values(vid_imgs_path + 'groundtruth_rect.txt')\n",
    "        \n",
    "        all_imgs_in_dir = [img_name for img_name in all_imgs_in_dir if '.DS_Store' not in img_name]\n",
    "        all_imgs_in_dir.sort(key=lambda f: int(re.sub('\\D', '', f)))\n",
    "        all_imgs_in_dir_with_index = [(index, img_name) for index, img_name in enumerate(all_imgs_in_dir)]\n",
    "        \n",
    "#         total_action_histories = []\n",
    "#         last_action_idxs = []\n",
    "#         num_steps_taken_in_each_episode = []\n",
    "        bboxes_from_tracking = []\n",
    "        action_probs = []\n",
    "        \n",
    "        if len(gt_values) == len(all_imgs_in_dir):\n",
    "            \n",
    "            '''in pretraining adnet with RL, for every video, we have to select 10 frames for tracking simulation'''\n",
    "            randomly_selected_indices = self.select_a_random_index_for_image_sampling(len(all_imgs_in_dir))\n",
    "            imgs_in_curr_selection = [all_imgs_in_dir[index] for index in randomly_selected_indices]\n",
    "            gt_values_of_curr_selection = [gt_values[index] for index in randomly_selected_indices]\n",
    "            \n",
    "            \n",
    "            action_history = np.zeros(shape = (1, 10, 11))\n",
    "            initial_finetune = True\n",
    "            curr_bbox_values = gt_values_of_curr_selection[0]\n",
    "            \n",
    "            for img_name in imgs_in_curr_selection:\n",
    "\n",
    "                img_path = vid_imgs_path + 'img/' + img_name\n",
    "                \n",
    "                \n",
    "                if initial_finetune:\n",
    "                    \n",
    "                    self.initial_finetune_adnet_pretrain_RL()\n",
    "                    \n",
    "                    initial_finetune = False\n",
    "                else:\n",
    "                    '''\n",
    "                    the resultant bboxes from previous self.tracking_procedure, has to be the input to the next iteration of the self.tracking_procedure\n",
    "                    '''\n",
    "                    stop_before_action_prob, action_history, curr_bbox_values = self.tracking_procedure(img_path, curr_bbox_values, action_history)\n",
    "                    bboxes_from_tracking.append(curr_bbox_values.copy())\n",
    "#                     num_steps_taken_in_each_episode.append(num_steps_taken)\n",
    "#                     total_action_histories.append(action_histories.copy())\n",
    "                    action_probs.append(stop_before_action_prob)\n",
    "            \n",
    "            return gt_boxes_of_curr_selection, action_probs, bboxes_from_tracking\n",
    "                \n",
    "                \n",
    "        else:\n",
    "            raise Exception(\"Sorry, there is a problem in reading frames and their correspoinding gt values\")\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    '''###################################'''\n",
    "    \n",
    "    def discount_rewards(self, rewards):\n",
    "        discounted_rewards = [((config.GAMMA ** idx) * reward) for idx, reward in enumerate(rewards)]\n",
    "        return discounted_rewards\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    '''###################################'''\n",
    "    \n",
    "    def get_adnet_RL_train_vars(self):\n",
    "        rl_train_vars = []\n",
    "        for variable in self.adnet_ckpt.model.trainable_variables:\n",
    "            if 'class' not in variable.name:\n",
    "                rl_train_vars.append(variable)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    '''###################################'''\n",
    "    \n",
    "    def in_each_adnet_RL_train_step(self, discounted_rewards, action_probs):\n",
    "        rl_train_vars = self.get_adnet_RL_train_vars()\n",
    "        for reward, prob in zip(discounted_rewards, action_probs):\n",
    "            with tf.GradientTape(persistent = True) as tape:\n",
    "                loss = -np.log(prob) * reward\n",
    "                         \n",
    "            \n",
    "            gradients = tape.gradient(loss, rl_train_vars)\n",
    "            self.adnet_ckpt.optimizer_2.apply_gradients(zip(gradients, rl_train_vars))\n",
    "        \n",
    "        \n",
    "                         \n",
    "        \n",
    "    '''###################################'''\n",
    "    \n",
    "    def perform_adnet_RL_training(self):\n",
    "        self.restore_recent_checkpoint()\n",
    "        all_train_vid_dirs = os.listdir(config.TRAIN_DATA_PATH)\n",
    "        random.shuffle(all_train_vid_dirs)\n",
    "        for vid_dir_name in all_train_vid_dirs:\n",
    "            vid_imgs_path = config.TRAIN_DATA_PATH + vid_dir_name + '/'\n",
    "            gt_values, action_probs, boxes_from_tracking = self.perfrom_tracking_and_finetune_weights(vid_imgs_path)\n",
    "            rewards = self.compute_z_score(gt_values.copy(), boxes_from_tracking.copy())\n",
    "            discount_rewards = self.discount_rewards(rewards.copy())\n",
    "            self.in_each_adnet_RL_train_step(discounted_rewards.copy(), action_probs.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_using_rl = Pretrain_RL()\n",
    "train_using_rl.perform_adnet_RL_training()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "########################################\n",
    "##### PRETRAINING USING RL ############\n",
    "########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Online_finetuning_RL(Pretrain_RL):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    \n",
    "    def finetune_online(self, img_path = None, bbox_gt_values = None, num_epochs = -1, new_samples = True):\n",
    "        action_history = np.zeros(shape = (config.ONLINE_FINETUNE_BATCH_SIZE, 1, 1, 110))\n",
    "        if new_samples:\n",
    "            pos_samples, neg_samples = self.generate_samples_obj.get_noisy_samples_action_and_class_label_for_single_gt_value(bbox_gt_values, img_path, 2000)\n",
    "        else: # if samples from the last 20 frames\n",
    "            pos_samples, neg_samples = self.get_samples()\n",
    "            \n",
    "        pos_samples = self.prepare_samples_for_training(pos_samples)\n",
    "        neg_samples = self.prepare_samples_for_training(neg_samples)\n",
    "        \n",
    "        p_imgs, p_action_labels, p_class_labels = self.get_batch_imgs_action_and_class_labels(pos_samples)\n",
    "        n_imgs, n_action_labels, n_class_labels = self.get_batch_imgs_action_and_class_labels(neg_samples)\n",
    "#         data_samples = self.prepare_samples_for_training(pos_samples + neg_samples)\n",
    "#         random.shuffle(data_samples)\n",
    "#         imgs, action_labels, _ = self.get_batch_imgs_action_and_class_labels(data_samples)\n",
    "        p_imgs_labels = list(zip(p_imgs, p_action_labels, p_class_labels))\n",
    "        n_imgs_labels = list(zip(n_imgs, n_action_labels, n_class_labels))   \n",
    "        for epoch in range(num_epochs):\n",
    "            random.sample(p_imgs_labels, config.ONLINE_FINETUNE_BATCH_SIZE)\n",
    "            random.sample(n_imgs_labels, config.ONLINE_FINETUNE_BATCH_SIZE)\n",
    "            p_batch_imgs, p_batch_a_labels, p_batch_c_labels = zip(*p_imgs_labels)\n",
    "            n_batch_imgs, n_batch_a_labels, n_batch_c_labels = zip(*n_imgs_labels)\n",
    "            \n",
    "            self.in_each_pretrain_adnet_step(p_batch_imgs, p_batch_a_labels, None, 'action', action_history)\n",
    "            self.in_each_pretrain_adnet_step(p_batch_imgs + n_batch_imgs, None, \n",
    "                                             [1] * config.ONLINE_FINETUNE_BATCH_SIZE + [0] * config.ONLINE_FINETUNE_BATCH_SIZE,\n",
    "                                             'class', action_history)\n",
    "            if epoch % 100 = 0:\n",
    "                self.adnet_ckpt_manager.save()\n",
    "            \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def redetect_the_target_position(self, bbox_values, img_path):\n",
    "        imgs = []\n",
    "        pos_samples, neg_samples = self.generate_samples_obj.get_noisy_samples_action_and_class_label_for_single_gt_value(curr_bbox, img_path, \n",
    "                                                                                             config.REDECTION_NUM_SAMPLES)\n",
    "        data_samples = self.preprate_samples_for_training(pos_samples + neg_samples)\n",
    "        bbox_values = [sample[0] for sample in data_samples]\n",
    "        for bbox_value in bbox_values:\n",
    "            imgs.append(self.adnet_pretrain_data_loader.get_image_using_bbox_values(bbox_values, img_path))\n",
    "        \n",
    "        _, class_logits = self.adnet_ckpt.model([imgs, np.zeros(config.REDECTION_NUM_SAMPLES, 1, 1, 110)], training = False)\n",
    "        class_scores = tf.nn.softmax(class_logits)\n",
    "        target_scores = [score[0] for score in class_scores]\n",
    "        max_score_index = np.argmax(target_scores)\n",
    "        max_bbox_value = bbox_values[max_score_index]\n",
    "        \n",
    "        return max_bbox_value\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    def get_samples(self):\n",
    "#         pos_samples = [sample for samples_from_each_frame in self.pos_samples_for_online_finetune for sample in samples_from_each_frame]\n",
    "#         neg_samples = [sample for samples_from_each_frame in self.neg_samples_for_online_finetune for sample in samples_from_each_frame]\n",
    "        pos_samples = []\n",
    "        neg_samples = []\n",
    "        assert len(self.pos_samples_for_online_finetune) == len(self.neg_samples_for_online_finetune)\n",
    "        num_pos_samples = len(self.pos_samples_for_online_finetune)\n",
    "        \n",
    "        \n",
    "        \n",
    "            \n",
    "        for index in range(20):\n",
    "            if index > num_pos_samples - 1:\n",
    "                break\n",
    "            else:\n",
    "                p_samples = self.pos_samples_for_online_finetune[(-index + 1)]\n",
    "                n_samples = self.pos_samples_for_online_finetune[(-index + 1)]\n",
    "\n",
    "                temp_p_samples=[sample for sample in p_samples]\n",
    "                temp_n_samples = [sample for sample in n_samples]\n",
    "\n",
    "                pos_samples.extend(temp_p_samples)\n",
    "                neg_samples.extend(temp_n_samples)\n",
    "        \n",
    "        return pos_samples, neg_samples\n",
    "\n",
    "    \n",
    "    \n",
    "    def update_weights(self):\n",
    "        self.finetune_online(None, None, 30, False)\n",
    "\n",
    "    \n",
    "    \n",
    "    def finetune_weights_online_using_rl(self):\n",
    "        self.restore_recent_checkpoint()\n",
    "        self.is_tracked = True\n",
    "        self.pos_samples_for_online_finetune = []\n",
    "        self.neg_samples_for_online_finetune = []\n",
    "        assert os.path.exists(config.ONLINE_TUNE_TEST_SEQ_PATH)\n",
    "        adnetwork\n",
    "        gt_values_txt_path = config.ONLINE_TUNE_TEST_SEQ_PATH + 'groundtruth_rect.txt'\n",
    "        gt_values = self.generate_samples_obj.get_gt_values(gt_values_txt_path)\n",
    "        \n",
    "        imgs_names = os.listdir(config.ONLINE_TUNE_TEST_SEQ_PATH)\n",
    "        imgs_names = [name for name in imgs_names if '.DS_Store' not in name]\n",
    "        action_histories = np.zeros(shape = (1, 10, 11))\n",
    "        \n",
    "        \n",
    "        for index, name in enumerate(imgs_names):\n",
    "            img_path = config.ONLINE_TUNE_TEST_SEQ_PATH + 'img/' + name\n",
    "            gt_values = gt_values[index]\n",
    "            if index == 0 or index == 1:\n",
    "                \n",
    "                self.initial_finetune_online(img_path, gt_values, 300, True)\n",
    "                curr_bbox = gt_values.copy()\n",
    "            \n",
    "            else:\n",
    "                action_probs, action_history, bbox_values = self.tracking_procedure(img_path, curr_bbox, action_histories)\n",
    "                cropped_img = self.generate_samples_obj.get_img_from_bbox_values(bbox_values, img_path)\n",
    "                action_logits, class_logits = self.adnet_ckpt.model([cropped_img, action_history], training = False)\n",
    "                confidence_score = tf.nn.softmax(class_logits)\n",
    "                if confidence_score[0][0] > 0.5: # targets score\n",
    "                    '''\n",
    "                    generate samples using the returned bbox_values and the current image\n",
    "                    all the samples contain bboxvalues, action label and class label\n",
    "                    '''\n",
    "                    \n",
    "                    pos_samples, neg_samples = self.generate_samples_obj.get_noisy_samples_action_and_class_label_for_single_gt_value(curr_bbox, img_path, \n",
    "                                                                                                                                config.ONLINE_FINETUNE_NUM_SAMPLES, \n",
    "                                                                                                                                config.RL_POS_SAMPS_THRESHOLD,\n",
    "                                                                                                                                config.RL_NEG_SAMPS_THRESHOLD)\n",
    "                    \n",
    "                    \n",
    "                    '''\n",
    "                    While tuning weights using online mode, we use generated samples from the last 20 frames, therefore if the len of samples saved is greater \n",
    "                    than 20, then we have to remove the first added item and the recent item get appended at the end.\n",
    "                    '''\n",
    "                    if len(self.pos_samples_for_online_finetune) > 19:\n",
    "                        del self.pos_samples_for_online_finetune[0]\n",
    "                    \n",
    "                    if len(self.neg_samples_for_online_finetuen) > 19:\n",
    "                        del self.neg_samples_for_online_finetuene[0]\n",
    "                        \n",
    "                    self.pos_samples_for_online_finetune.extend([pos_samples])\n",
    "                    self.neg_samples_for_online_finetune.extend([neg_samples])\n",
    "                    curr_bbox = bbox_values.copy()\n",
    "                    action_histories = action_history.copy()\n",
    "                else:\n",
    "                    '''perform redetection'''\n",
    "                    self.is_tracked = False\n",
    "                    max_bbox_value = self.redetect_the_target_position()\n",
    "                    curr_bbox = max_bbox_value.copy()\n",
    "                    action_histories = action_history.copy()\n",
    "                    \n",
    "            if index % config.ONLINE_FINETUNE_FREQUENCY == 0 and index > 19:\n",
    "                self.update_weights()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
